# ğŸ¥ Cura AI RAG Frontend + Backend

Complete working medical assistant with **Pinecone-powered RAG** and **real-time voice support**.

---

## ğŸš€ Quick Start

### 1. Start Backend + Frontend

```pwsh
cd d:\curaai-platform\curai-backend
.\start-rag-demo.ps1
```

This will:
- Launch Fastify backend on `http://localhost:3000`
- Open `frontend-test.html` in your browser
- Connect to your Pinecone index with embedded medical knowledge

### 2. Manual Start

**Backend:**
```pwsh
cd d:\curaai-platform\curai-backend
npm run dev
```

**Frontend:**
Open `frontend-test.html` in your browser (double-click or drag to browser).

---

## ğŸ§ª Testing the RAG System

### Text-Based RAG Search

1. **Default mode** is RAG Search (text input)
2. Type a medical question: 
   - *"What are the symptoms of type 2 diabetes?"*
   - *"How do you treat acute myocardial infarction?"*
   - *"Explain the pathophysiology of heart failure"*
3. Click **Search** or press `Enter`
4. View:
   - **AI Response** (generated from retrieved context)
   - **Sources** card showing matching medical Q&A pairs from Pinecone
   - **Match scores** (cosine similarity %)

### Real-Time Voice Mode

1. Click **Real-Time** radio button
2. Click **Start Real-Time Voice Mode**
3. Allow microphone access
4. Speak your medical question
5. System will:
   - Transcribe audio (ASR)
   - Run RAG search
   - Generate response
   - Synthesize speech (TTS)

### Emergency Mode

1. Check **ğŸš¨ Emergency Mode** checkbox
2. Ask emergency-related question
3. Response will prioritize:
   - Immediate life-saving actions
   - "CALL 911" prompt
   - Lower temperature (more deterministic)
   - Emergency-optimized model

---

## ğŸ“Š Backend Endpoints

### RAG Search
```http
POST http://localhost:3000/api/search
Content-Type: application/json

{
  "query": "What are the symptoms of diabetes?",
  "emergency": false,
  "topK": 5
}
```

**Response:**
```json
{
  "success": true,
  "sessionId": "session-1234567890",
  "query": "What are the symptoms of diabetes?",
  "sources": [
    {
      "id": "medqa_12345",
      "score": 0.89,
      "text": "Question: What are common symptoms...",
      "metadata": { "dataset": "medqa", "source": "medqa_train" }
    }
  ],
  "response": "Type 2 diabetes symptoms include...",
  "emergency": false,
  "model": "meta-llama/llama-3.3-70b-instruct",
  "timestamp": "2025-11-16T..."
}
```

### Pinecone Stats
```http
GET http://localhost:3000/api/embed/stats
```

**Response:**
```json
{
  "success": true,
  "indexName": "default",
  "vectorCount": 150000,
  "dimension": 1536,
  "namespaces": 0,
  "host": "default-xxx.svc.pinecone.io"
}
```

### Health Check
```http
GET http://localhost:3000/health
```

### WebSocket Real-Time
```
ws://localhost:3000/ws/realtime?sessionId=xyz
```

---

## ğŸ”§ Architecture

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                    Frontend (HTML/JS)                       â”‚
â”‚  - Text input â†’ POST /api/search                           â”‚
â”‚  - Voice input â†’ WebSocket /ws/realtime                    â”‚
â”‚  - Display: Response + Sources + Metadata                  â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                            â”‚
                            â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                  Fastify Backend (Node.js)                  â”‚
â”‚  Routes:                                                     â”‚
â”‚    - POST /api/search      â†’ RAG pipeline                   â”‚
â”‚    - GET  /api/embed/stats â†’ Pinecone stats                â”‚
â”‚    - WS   /ws/realtime     â†’ Voice streaming               â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                            â”‚
                            â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                      RAG Service                            â”‚
â”‚  1. Generate query embedding (OpenRouter)                   â”‚
â”‚  2. Retrieve top-K vectors (Pinecone)                      â”‚
â”‚  3. Extract metadata (Q/A/context)                         â”‚
â”‚  4. Build prompt with retrieved context                    â”‚
â”‚  5. Generate response (OpenRouter LLM)                     â”‚
â”‚  6. Return: response + sources + scores                    â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
              â”‚                              â”‚
              â–¼                              â–¼
   â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”       â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
   â”‚  OpenRouter API     â”‚       â”‚  Pinecone Index     â”‚
   â”‚  - Embeddings       â”‚       â”‚  - 260k vectors     â”‚
   â”‚  - LLM Generation   â”‚       â”‚  - Medical Q&A      â”‚
   â”‚  - Streaming        â”‚       â”‚  - Metadata rich    â”‚
   â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜       â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

---

## ğŸ§  RAG Pipeline Details

### Step 1: Query Embedding
- Model: `openai/text-embedding-3-small` (1536 dims)
- Input: User question text
- Output: Dense vector representation

### Step 2: Dense Retrieval (Pinecone)
- Query vector against 260k medical Q&A pairs
- Cosine similarity search
- Retrieve top-K (default 5) matches
- Include metadata: question, answer, source, dataset

### Step 3: Context Building
- Extract text from matched vectors
- Format: `[Source N] Question: ... Answer: ...`
- Combine top-3 (configurable) sources

### Step 4: LLM Generation
- System prompt: Safety rules + first-aid guidance
- User prompt: Context + Question + Instructions
- Model: `meta-llama/llama-3.3-70b-instruct` (default)
- Temperature: 0.3 (0.1 for emergency)

### Step 5: Response Assembly
- AI-generated answer
- Source citations with scores
- Metadata (timestamps, model, emergency flag)

---

## ğŸ“ Key Files

```
curai-backend/
â”œâ”€â”€ frontend-test.html          â† Open this in browser
â”œâ”€â”€ start-rag-demo.ps1           â† Quick launcher script
â”œâ”€â”€ src/
â”‚   â”œâ”€â”€ server.js                â† Fastify server
â”‚   â”œâ”€â”€ routes/
â”‚   â”‚   â”œâ”€â”€ search.js            â† POST /api/search (RAG)
â”‚   â”‚   â””â”€â”€ embed.js             â† GET /api/embed/stats
â”‚   â”œâ”€â”€ services/
â”‚   â”‚   â”œâ”€â”€ rag.js               â† RAG pipeline orchestration
â”‚   â”‚   â”œâ”€â”€ pinecone.js          â† Vector search
â”‚   â”‚   â”œâ”€â”€ openrouter.js        â† LLM + embeddings
â”‚   â”‚   â”œâ”€â”€ embedding-helper.js  â† Embedding wrapper
â”‚   â”‚   â”œâ”€â”€ asr.js               â† Speech-to-text
â”‚   â”‚   â””â”€â”€ tts.js               â† Text-to-speech
â”‚   â”œâ”€â”€ middleware/
â”‚   â”‚   â”œâ”€â”€ emergency-detect.js  â† Keyword detection
â”‚   â”‚   â””â”€â”€ safety.js            â† Response sanitization
â”‚   â””â”€â”€ ws/
â”‚       â””â”€â”€ ws-server.js         â† WebSocket handler
â”œâ”€â”€ embed-datasets.js            â† Dataset embedding script
â”œâ”€â”€ check-pinecone-stats.js      â† Check vector count
â””â”€â”€ .env                         â† API keys + config
```

---

## ğŸ› Troubleshooting

### "Backend Offline" in Frontend

**Check backend is running:**
```pwsh
curl http://localhost:3000/health
```

**Start backend manually:**
```pwsh
cd d:\curaai-platform\curai-backend
npm run dev
```

### "No matches found in Pinecone"

**Check vector count:**
```pwsh
node check-pinecone-stats.js
```

**If 0 vectors, run embedding:**
```pwsh
.\start-embedding.ps1
```

### CORS Errors

Frontend must be opened via:
- File protocol (`file:///...`)
- Or served from same origin

Backend already has CORS enabled for `localhost`.

### Real-Time Mode Not Working

1. Check WebSocket connection in browser console
2. Ensure microphone permissions granted
3. Check backend logs for WebSocket events

---

## ğŸ¯ Example Queries

### General Medical Questions
- "What causes high blood pressure?"
- "Explain the difference between Type 1 and Type 2 diabetes"
- "What are the risk factors for stroke?"

### Clinical Scenarios
- "A patient presents with chest pain radiating to the left arm. What should I consider?"
- "Describe the management of acute asthma exacerbation"
- "What are the diagnostic criteria for sepsis?"

### Emergency Situations (check Emergency Mode)
- "Someone is unconscious and not breathing. What do I do?"
- "How do I perform CPR?"
- "What are the steps for managing severe bleeding?"

---

## ğŸ“ˆ Performance Metrics

- **Query latency:** ~2-4 seconds (embedding + retrieval + generation)
- **Embedding generation:** ~200ms per query
- **Pinecone retrieval:** ~300-500ms for top-5
- **LLM generation:** ~1-3 seconds (streaming available)
- **Total vectors:** 260,000 (capped for 2GB free tier)
- **Storage used:** ~1.9 GB

---

## ğŸ” Security Notes

- **Never use for actual medical diagnosis**
- **Always recommend consulting healthcare professionals**
- **Emergency mode includes 911 prompt**
- **Response safety middleware active**
- **Input sanitization enabled**

---

## ğŸš€ Next Steps

1. âœ… Verify backend running: `http://localhost:3000/health`
2. âœ… Check Pinecone stats: `node check-pinecone-stats.js`
3. âœ… Open `frontend-test.html` in browser
4. âœ… Test RAG search with medical question
5. âœ… Review sources + match scores
6. âœ… Try emergency mode
7. âœ… Test real-time voice mode (optional)

---

**Built with:**
- **Fastify** - Fast Node.js web framework
- **Pinecone** - Vector database (serverless)
- **OpenRouter** - LLM API aggregator
- **Embeddings**: text-embedding-3-small (OpenAI)
- **LLM**: llama-3.3-70b-instruct (Meta)
- **Datasets**: MedQA, BioASQ, MedMCQA

ğŸ¥ **Ready for medical AI assistance!**
