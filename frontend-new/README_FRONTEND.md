# ğŸ©º Cura AI Frontend

**Version:** v1.0.0 (Buildathon MVP)  
**Framework:** Next.js 14 with App Router  
**Real-time Features:** Voice Streaming, Video Streaming, Emergency Detection

---

## ğŸ“‹ Table of Contents

- [Overview](#overview)
- [Features](#features)
- [Tech Stack](#tech-stack)
- [Project Structure](#project-structure)
- [Setup & Installation](#setup--installation)
- [Environment Variables](#environment-variables)
- [Usage](#usage)
- [WebSocket Events](#websocket-events)
- [Architecture](#architecture)
- [Development](#development)
- [Production Build](#production-build)
- [Troubleshooting](#troubleshooting)

---

## ğŸ¯ Overview

Cura AI is a real-time medical emergency assistant frontend that provides:
- **Text Chat Mode**: ChatGPT-style interface for medical queries
- **Realtime Voice Mode**: Live voice interaction with ASR transcription
- **Video Streaming**: Optional camera feed for visual context
- **Emergency Detection**: Smart alerts with guided instructions
- **Responsive UI**: Clean, medical-themed interface

---

## âœ¨ Features

### ğŸ’¬ Chat Interface
- Real-time text messaging
- Streaming LLM responses
- Message history with timestamps
- Smooth animations

### ğŸ¤ Realtime Voice Mode
- WebSocket-based voice streaming
- Live Whisper ASR transcription
- Waveform visualization
- Auto-transcription subtitles
- Voice activity detection

### ğŸ“¹ Video Support (Optional)
- Camera preview with live feed
- Automatic frame capture (320Ã—240 JPEG)
- 1-2 second frame interval
- Device selection

### ğŸš¨ Emergency Alerts
- Real-time emergency state detection
- Visual alert cards with guidance
- One-tap emergency calling (108/112)
- Animated alert indicators
- State-based instructions

### âš™ï¸ Settings
- Light/Dark mode toggle
- Audio device selection
- Video device selection
- Subtitle preferences
- Persistent settings storage

---

## ğŸ›  Tech Stack

- **Framework**: Next.js 14 (App Router)
- **Language**: TypeScript
- **Styling**: Tailwind CSS + ShadCN UI
- **State Management**: Zustand (with persistence)
- **Animations**: Framer Motion
- **Icons**: Lucide React
- **Audio/Video**: Browser MediaRecorder API
- **WebSocket**: Native WebSocket API
- **LLM Provider**: OpenRouter (temporary)

---

## ğŸ“ Project Structure

```
frontend/
â”œâ”€â”€ app/
â”‚   â”œâ”€â”€ page.tsx                    # Landing page
â”‚   â”œâ”€â”€ layout.tsx                  # Root layout
â”‚   â”œâ”€â”€ chat/[id]/page.tsx          # Main chat interface
â”‚   â”œâ”€â”€ signin/page.tsx             # Sign-in page
â”‚   â”œâ”€â”€ signup/page.tsx             # Sign-up page
â”‚   â””â”€â”€ dashboard/                  # Dashboard pages
â”œâ”€â”€ components/
â”‚   â”œâ”€â”€ chat/
â”‚   â”‚   â”œâ”€â”€ ChatWindow.tsx          # Message display
â”‚   â”‚   â”œâ”€â”€ MessageBubble.tsx       # Individual messages
â”‚   â”‚   â””â”€â”€ ChatInputBar.tsx        # Input with controls
â”‚   â”œâ”€â”€ realtime/
â”‚   â”‚   â”œâ”€â”€ WaveformVisualizer.tsx  # Audio visualization
â”‚   â”‚   â”œâ”€â”€ VideoPreview.tsx        # Camera preview
â”‚   â”‚   â””â”€â”€ RealtimeModeUI.tsx      # Realtime overlay
â”‚   â”œâ”€â”€ emergency/
â”‚   â”‚   â””â”€â”€ EmergencyAlertCard.tsx  # Emergency alerts
â”‚   â”œâ”€â”€ layout/
â”‚   â”‚   â”œâ”€â”€ SidebarChatList.tsx     # Chat history sidebar
â”‚   â”‚   â””â”€â”€ TopBar.tsx              # Top navigation
â”‚   â”œâ”€â”€ settings/
â”‚   â”‚   â””â”€â”€ SettingsDialog.tsx      # Settings modal
â”‚   â””â”€â”€ ui/                         # ShadCN components
â”œâ”€â”€ store/
â”‚   â””â”€â”€ chatStore.ts                # Zustand global state
â”œâ”€â”€ lib/
â”‚   â”œâ”€â”€ websocket.ts                # WebSocket manager
â”‚   â”œâ”€â”€ audio.ts                    # Audio recording utilities
â”‚   â”œâ”€â”€ video.ts                    # Video capture utilities
â”‚   â””â”€â”€ utils.ts                    # Helper functions
â”œâ”€â”€ styles/
â”‚   â””â”€â”€ globals.css                 # Global styles
â”œâ”€â”€ public/                         # Static assets
â”œâ”€â”€ .env.local                      # Environment variables
â”œâ”€â”€ .env.example                    # Environment template
â””â”€â”€ README.md                       # This file
```

---

## ğŸš€ Setup & Installation

### Prerequisites
- Node.js 18+ and npm/yarn/pnpm
- Modern browser with WebRTC support
- Microphone and camera (for realtime features)

### Installation Steps

1. **Clone the repository**
```bash
git clone https://github.com/yourusername/cura-ai-frontend.git
cd cura-ai-frontend
```

2. **Install dependencies**
```bash
npm install --legacy-peer-deps
# or
yarn install
# or
pnpm install
```

3. **Set up environment variables**
```bash
cp .env.example .env.local
```

Edit `.env.local` and add your configuration:
```env
NEXT_PUBLIC_SUPABASE_URL=your-supabase-url
NEXT_PUBLIC_SUPABASE_ANON_KEY=your-supabase-anon-key
NEXT_PUBLIC_OPENROUTER_KEY=your-openrouter-api-key
NEXT_PUBLIC_API_BASE_URL=http://localhost:8000
NEXT_PUBLIC_WS_BASE_URL=ws://localhost:8000
```

4. **Run the development server**
```bash
npm run dev
```

5. **Open your browser**
Navigate to [http://localhost:3000](http://localhost:3000)

---

## ğŸ” Environment Variables

| Variable | Description | Required |
|----------|-------------|----------|
| `NEXT_PUBLIC_SUPABASE_URL` | Supabase project URL | Yes |
| `NEXT_PUBLIC_SUPABASE_ANON_KEY` | Supabase anonymous key | Yes |
| `NEXT_PUBLIC_OPENROUTER_KEY` | OpenRouter API key | Yes |
| `NEXT_PUBLIC_API_BASE_URL` | Backend REST API URL | Yes |
| `NEXT_PUBLIC_WS_BASE_URL` | Backend WebSocket URL | Yes |
| `NEXT_PUBLIC_APP_NAME` | Application name | No |
| `NEXT_PUBLIC_APP_VERSION` | Application version | No |

---

## ğŸ“– Usage

### Text Chat Mode

1. Click "Get Started" or create a new chat
2. Type your medical query in the input box
3. Press Enter or click Send
4. Receive streaming AI responses

### Realtime Voice Mode

1. Click the **Microphone** button in the input bar
2. Allow microphone permissions
3. Speak naturally - see live transcription
4. Receive real-time AI guidance
5. Click **Stop** to exit realtime mode

### Video Mode (Optional)

1. Enable Realtime mode first
2. Click the **Camera** button
3. Allow camera permissions
4. Video frames sent automatically

### Emergency Alerts

- System detects emergency states automatically
- Red alert card appears with guidance
- Follow instructions carefully
- Click **Call 108/112** for immediate help

---

## ğŸ“¡ WebSocket Events

### Client â†’ Server

| Event | Description | Payload |
|-------|-------------|---------|
| `audio_chunk` | PCM16 audio data | `{ audio: number[], format, sampleRate }` |
| `video_frame` | JPEG frame data | `{ frame: string, format }` |
| `text_message` | Text message | `{ text: string }` |
| `user_intent` | User action/intent | `{ intent: string }` |

### Server â†’ Client

| Event | Description | Payload |
|-------|-------------|---------|
| `transcription` | ASR result | `{ text: string }` |
| `intermediate_response` | Streaming chunk | `{ text: string }` |
| `final_response` | Complete response | `{ text: string }` |
| `emergency_state_update` | Emergency detected | `{ state: string, message: string }` |
| `conversation_metadata` | Session info | `{ ... }` |
| `error` | Error message | `{ error: string }` |

---

## ğŸ— Architecture

```mermaid
graph TD
    A[User Interface] --> B[Zustand Store]
    B --> C[WebSocket Manager]
    B --> D[Audio Recorder]
    B --> E[Video Capture]
    
    C --> F[Backend WebSocket]
    F --> G[ASR Service]
    F --> H[LLM Service]
    F --> I[Emergency Detector]
    
    G --> C
    H --> C
    I --> C
    
    D --> J[MediaRecorder]
    E --> K[getUserMedia]
    
    J --> C
    K --> C
```

### Data Flow

**Text Mode:**
```
User Input â†’ REST API â†’ RAG â†’ LLM â†’ Streaming Response â†’ UI
```

**Realtime Mode:**
```
Microphone â†’ Audio Chunks â†’ WebSocket â†’ ASR â†’ LLM â†’ Streaming â†’ UI
```

**Emergency Detection:**
```
Backend Analysis â†’ WebSocket Event â†’ Emergency State â†’ Alert UI
```

---

## ğŸ’» Development

### Available Scripts

```bash
# Development server
npm run dev

# Build for production
npm run build

# Start production server
npm start

# Run linter
npm run lint
```

### Key Development Files

- **State Management**: `store/chatStore.ts`
- **WebSocket Logic**: `lib/websocket.ts`
- **Audio Processing**: `lib/audio.ts`
- **Video Capture**: `lib/video.ts`
- **Main Chat Page**: `app/chat/[id]/page.tsx`

---

## ğŸš¢ Production Build

```bash
# Build the project
npm run build

# Start production server
npm start

# Or deploy to Vercel
vercel deploy
```

### Environment Setup

Ensure all production environment variables are set in your hosting platform.

---

## ğŸ› Troubleshooting

### Microphone Not Working

- Check browser permissions
- Verify device selection in Settings
- Try different audio input device

### Camera Not Working

- Allow camera permissions
- Check device selection in Settings
- Ensure camera not in use by other apps

### WebSocket Connection Failed

- Verify backend is running
- Check `NEXT_PUBLIC_WS_BASE_URL`
- Ensure CORS is configured on backend

### Supabase Auth Issues

- Verify Supabase credentials in `.env.local`
- Check Supabase project settings
- Ensure auth is enabled in Supabase dashboard

### Build Errors

```bash
# Clear cache and reinstall
rm -rf node_modules .next package-lock.json
npm install --legacy-peer-deps
npm run build
```

---

## ğŸ“ License

MIT License - See LICENSE file for details

---

## ğŸ¤ Contributing

Contributions welcome! Please follow:
1. Fork the repository
2. Create a feature branch
3. Make your changes
4. Submit a pull request

---

## ğŸ“§ Support

For issues or questions:
- GitHub Issues: [Create an issue](https://github.com/yourusername/cura-ai/issues)
- Email: support@cura-ai.com

---

## ğŸ™ Acknowledgments

- Next.js Team
- ShadCN UI
- Framer Motion
- OpenRouter
- Supabase

---

**Built with â¤ï¸ for Buildathon MVP**
